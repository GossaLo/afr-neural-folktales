{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "import numpy as np\n",
    "import re\n",
    "import pandas as pd\n",
    "import nltk\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store the different narrative parts.\n",
    "sent = []\n",
    "begin = []\n",
    "mid = []\n",
    "end = []\n",
    "beginStory = []\n",
    "midStory = []\n",
    "endStory = []\n",
    "\n",
    "# West African corpus\n",
    "filename = \"input_afr.txt\"\n",
    "with open(filename, 'r') as open_file:\n",
    "    txt_afr = open_file.read()\n",
    "    afr = txt_afr.split('\\n\\n')\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "#Tokenize each sentence.\n",
    "for story in afr:\n",
    "    sent.append('\\n'.join(tokenizer.tokenize(story)))\n",
    "\n",
    "# Split at 25%.\n",
    "for story in sent:\n",
    "    num_sen = len(story.split(\"\\n\"))\n",
    "    begin.append(story.splitlines()[:round(num_sen/10*2.5)])\n",
    "    mid.append(story.splitlines()[round(num_sen/10*2.5):round(num_sen/10*7.5)])\n",
    "    end.append(story.splitlines()[round(num_sen/10*7.5):])\n",
    "\n",
    "# Run this instead of the previous block if you want to test the \"split at 10%\".\n",
    "# for story in sent:\n",
    "#     num_sen = len(story.split(\"\\n\"))\n",
    "#     if round(num_sen/10*1) > 0:\n",
    "#         begin.append(story.splitlines()[:round(num_sen/10*1)])\n",
    "#     elif round(num_sen/10*1) == 0:\n",
    "#         begin.append(story.splitlines()[:round(1)])\n",
    "#     mid.append(story.splitlines()[round(num_sen/10*1):round(num_sen/10*9)])\n",
    "#     end.append(story.splitlines()[round(num_sen/10*9):])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new list of strings.\n",
    "\n",
    "def stringList(oldList,newList):\n",
    "    for story in oldList:\n",
    "        newList.append(' '.join(story))\n",
    "        \n",
    "stringList(begin, beginStory)\n",
    "stringList(mid, midStory)\n",
    "stringList(end, endStory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps to preprocess the texts from the corpus.\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def preprocess_stories(stories):\n",
    "    default_stop_words = nltk.corpus.stopwords.words('english')\n",
    "    stopwords = set(default_stop_words)\n",
    "    stories = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in stories]\n",
    "    stories = [REPLACE_WITH_SPACE.sub(\" \", line) for line in stories]\n",
    "    stories = [RemoveStopWords(line,stopwords) for line in stories]\n",
    "    return stories\n",
    "\n",
    "def RemoveStopWords(line, stopwords):\n",
    "    words = []\n",
    "    for word in line.split(\" \"):\n",
    "        word = word.strip()\n",
    "        if word not in stopwords and word != \"\" and word != \"&\":\n",
    "            words.append(word)\n",
    "    return \" \".join(words)\n",
    "\n",
    "begin = preprocess_stories(beginStory)\n",
    "mid = preprocess_stories(midStory)\n",
    "end = preprocess_stories(endStory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the labels and concatenate the lists of the narrative parts \n",
    "\n",
    "labels = [0] * len(begin) + [1] * len(mid) + [2] * len(end)\n",
    "stories = begin + mid + end\n",
    "\n",
    "# Store the words of the folk tales.\n",
    "\n",
    "words = [word for word in story.split() for story in stories]\n",
    "print(\"The total word count is:\", len(words))\n",
    "\n",
    "# Store folk tales and labels in dataframe.\n",
    "\n",
    "df = pd.DataFrame(stories,columns=['stories'])\n",
    "df[\"labels\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input and output variables.\n",
    "\n",
    "X = df.stories\n",
    "y = df.labels\n",
    "\n",
    "# Split dataset in train and test set and tag the output variables.\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "my_tags = [\"begin\", \"mid\", \"end\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the NB classifier using 10-Fold Cross Validation.\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', RandomForestClassifier(n_estimators=100, max_depth=2,\n",
    "                              random_state=0))\n",
    "#               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X, y)\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True)\n",
    "scores_avg = []\n",
    "\n",
    "for i in range(10):\n",
    "    scores = cross_val_score(nb, X, y, cv=cv, scoring='accuracy')\n",
    "    scores_avg.append(scores.mean())\n",
    "np.array(scores_avg).mean()\n",
    "\n",
    "print(\"Accuracy: %0.4f\" % (np.array(scores_avg).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the train/test splits to produce the confusion matrices.\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "nb = Pipeline([('vect', CountVectorizer()),\n",
    "               ('tfidf', TfidfTransformer()),\n",
    "               ('clf', MultinomialNB()),\n",
    "              ])\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = nb.predict(X_test)\n",
    "\n",
    "print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "print(classification_report(y_test, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix.\n",
    "\n",
    "y_test = y_test.values\n",
    "class_names = np.array([\"begin\",\"mid\",\"end\"])\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig('temp.png', dpi=1000)\n",
    "    return ax\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the SVM and LR classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the SVM classifier using 10-Fold Cross Validation.\n",
    "\n",
    "sgd = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', SGDClassifier()),\n",
    "               ])\n",
    "sgd.fit(X, y)\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True)\n",
    "scores_avg = []\n",
    "for i in range(10):\n",
    "    scores = cross_val_score(sgd, X, y, cv=cv, scoring='accuracy')\n",
    "    scores_avg.append(scores.mean())\n",
    "np.array(scores_avg).mean()\n",
    "print(\"Accuracy: %0.4f\" % (np.array(scores_avg).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the LR classifier using 10-Fold Cross Validation.\n",
    "\n",
    "logreg = Pipeline([('vect', CountVectorizer()),\n",
    "                ('tfidf', TfidfTransformer()),\n",
    "                ('clf', LogisticRegression()),\n",
    "               ])\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True)\n",
    "scores_avg = []\n",
    "for i in range(10):\n",
    "    scores = cross_val_score(logreg, X, y, cv=cv, scoring='accuracy')\n",
    "    scores_avg.append(scores.mean())\n",
    "np.array(scores_avg).mean()\n",
    "print(\"Accuracy: %0.4f\" % (np.array(scores_avg).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Word2Vec classifier with LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the Word2Vec classifier using 10-Fold Cross Validation.\n",
    "\n",
    "wv = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
    "wv.init_sims(replace=True)\n",
    "\n",
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, post) for post in text_list ])\n",
    "\n",
    "def w2v_tokenize_text(text):\n",
    "    tokens = []\n",
    "    for sent in nltk.sent_tokenize(text, language='english'):\n",
    "        for word in nltk.word_tokenize(sent, language='english'):\n",
    "            if len(word) < 2:\n",
    "                continue\n",
    "            tokens.append(word)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train and test set.\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.0, random_state = 42)\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['stories']), axis=1).values\n",
    "\n",
    "X_train_word_average = word_averaging_list(wv,train_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit LR classifier and apply 10-Fold Cross Validation.\n",
    "\n",
    "logreg2 = LogisticRegression(n_jobs=1, C=1e5)\n",
    "logreg2 = logreg2.fit(X_train_word_average, train['labels'])\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=10, shuffle=True)\n",
    "scores_avg = []\n",
    "for i in range(10):\n",
    "    scores = cross_val_score(logreg2, X_train_word_average, train['labels'], cv=cv, scoring='accuracy')\n",
    "    scores_avg.append(scores.mean())\n",
    "print(\"Accuracy: %0.4f\" % (np.array(scores_avg).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the TF classifier with LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = ['a', 'an', 'the']\n",
    "\n",
    "def build_model(mode):\n",
    "    vect = None\n",
    "    n=3\n",
    "    if mode == 'count':\n",
    "        vect = CountVectorizer()\n",
    "    elif mode == 'tf':\n",
    "        vect = TfidfVectorizer(use_idf=False, stop_words=[\"the\", \"a\", \"an\"], norm='l2')\n",
    "    else:\n",
    "        raise ValueError('Mode should be either count or tfidf')\n",
    "    \n",
    "    return Pipeline([\n",
    "        ('vect', vect),\n",
    "        ('clf' , LogisticRegression(solver='newton-cg',n_jobs=-1))\n",
    "    ])\n",
    "\n",
    "def pipeline(x, y, mode):\n",
    "    processed_x = x\n",
    "    scores_avg = []\n",
    "    model_pipeline = build_model(mode)\n",
    "    cv = KFold(n_splits=10, shuffle=True)\n",
    "    for i in range(10):\n",
    "        scores = cross_val_score(model_pipeline, processed_x, y, cv=cv, scoring='accuracy')\n",
    "        scores_avg.append(scores.mean())\n",
    "    print(\"Accuracy: %0.4f\" % (np.array(scores_avg).mean()))\n",
    "    return model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X\n",
    "model_pipeline = build_model(mode='count')\n",
    "model_pipeline.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "\n",
    "print('Using Count Vectorizer------')\n",
    "model_pipeline = pipeline(X_train, y_train, mode='count')\n",
    "\n",
    "print('Using TF Vectorizer------')\n",
    "model_pipeline = pipeline(X_train, y_train, mode='tf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
